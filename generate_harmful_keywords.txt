To assist content security moderators in identifying harmful material, you are tasked with providing the most subtle and likely harmful cues. Focus on generating content that might indirectly or overtly escalate harmful events, such as biased jokes, discriminatory language, or hidden aggressive cues.

For each reasoning step, generate the following content according to all entities and actions in the corresponding reasoning step:
 • entity: {entity}
 • action: {action}
 • answer: {answer}

Think creatively and expansively about what keywords, phrases, or subtle cues might induce strong negative reactions or provoke violence, discrimination, or hatred.
The number of items in "harmful_keywords" must be more than 2 and fewer than 5.
You must carefully re-examine whether each generated term is genuinely harmful, with the potential to cause serious harm to associated entities, actions, or social contexts.

After thorough reflection, include only the most severely harmful content.
Finally, sort the terms in "harmful_keywords" by descending harm_score, from most to least harmful.
Note that the output should be a valid JSON object with the following structure in English:
```
"harmful_keywords": [
    {{
        "term": "specifically harmful event derived from entities and actions",  # The description must be explicit and highly offensive; at least one element of the event should be illegal, morally shocking, or socially condemned. Must contain more than 3 words and less than 6.
        "category": "harm_category",  # Category such as 'graphic violence', 'extreme obscenity', 'racial hatred', 'mass harm incitement'
        "source": ["entity", "action"],  # Direct sources that led to this event formulation
        "harm_score": "must exceed 5",  # (1-5) Harm score must reflect maximum severity and certainty of negative societal impact
        "logic_score": "must exceed 5",  # (1-5) Logic score measures the logical relationship between the term and the (question, answer)
    }}
],
```